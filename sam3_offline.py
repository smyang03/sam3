"""
SAM3 YOLO Dataset Creator - Offline Server Version
ì˜¤í”„ë¼ì¸ ë¦¬ëˆ…ìŠ¤ ì„œë²„ìš© (GPUë³„ ë¶„ì‚° ì‹¤í–‰)
"""

import os
import sys
import time
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import json

# ========== ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì • ==========
# HuggingFace ì˜¤í”„ë¼ì¸ ëª¨ë“œ ê°•ì œ ì„¤ì •
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

# ========== CUDA ê²½ë¡œ ìë™ ì„¤ì • (OS ê°ì§€) ==========
if "CUDA_PATH" not in os.environ:
    import platform
    system = platform.system()
    
    if system == "Windows":
        # Windows CUDA ê²½ë¡œ
        possible_paths = [
            r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6",
            r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4",
            r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1",
            r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8",
        ]
    else:  # Linux
        # Linux CUDA ê²½ë¡œ
        possible_paths = [
            "/usr/local/cuda-12.6",
            "/usr/local/cuda-12.4",
            "/usr/local/cuda-12.1",
            "/usr/local/cuda-11.8",
            "/usr/local/cuda",
        ]
    
    # ì¡´ì¬í•˜ëŠ” ê²½ë¡œ ì°¾ê¸°
    for cuda_path in possible_paths:
        if os.path.exists(cuda_path):
            os.environ["CUDA_PATH"] = cuda_path
            break
    else:
        # CUDA ì—†ì–´ë„ ì§„í–‰ (Triton ì˜¤ë¥˜ ë°©ì§€ìš© ë¹ˆ ë¬¸ìì—´)
        os.environ["CUDA_PATH"] = ""

import certifi
os.environ['SSL_CERT_FILE'] = certifi.where()

# SAM3 imports
from sam3 import build_sam3_image_model
from sam3.train.data.collator import collate_fn_api as collate
from sam3.model.utils.misc import copy_data_to_device
from sam3.train.data.sam3_image_dataset import (
    InferenceMetadata, FindQueryLoaded, 
    Image as SAMImage, Datapoint
)
from sam3.train.transforms.basic_for_api import (
    ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI
)
from sam3.eval.postprocessors import PostProcessImage
from sam3.eval.postprocessors_classwise import (
    PostProcessImageWithClassThresholds,
    create_postprocessor_from_config
)

# Global counter for query IDs
GLOBAL_COUNTER = 1


def recursive_to_device(obj, device):
    """ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í…ì„œë¥¼ deviceë¡œ ì´ë™"""
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: recursive_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [recursive_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(recursive_to_device(item, device) for item in obj)
    elif hasattr(obj, '__dict__'):
        for attr_name in dir(obj):
            if not attr_name.startswith('_'):
                try:
                    attr = getattr(obj, attr_name)
                    if isinstance(attr, (torch.Tensor, dict, list, tuple)) or hasattr(attr, '__dict__'):
                        setattr(obj, attr_name, recursive_to_device(attr, device))
                except (AttributeError, TypeError):
                    pass
        return obj
    else:
        return obj


def setup_environment(gpu_id=None, device='auto'):
    """
    í™˜ê²½ ì„¤ì •
    
    Args:
        gpu_id: GPU ì¸ë±ìŠ¤ (0, 1, 2, ...) - Noneì´ë©´ ìë™ ì„ íƒ
        device: 'auto', 'cuda', 'cpu'
    """
    print("=" * 60)
    print("í™˜ê²½ ì„¤ì • ì¤‘...")
    print("=" * 60)
    
    # GPU ì¸ë±ìŠ¤ ì§€ì •
    if gpu_id is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        print(f"âœ“ GPU ì¸ë±ìŠ¤ ì§€ì •: {gpu_id}")
        device = 'cuda'  # GPU ì§€ì • ì‹œ ìë™ìœ¼ë¡œ CUDA ëª¨ë“œ
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"âœ“ CUDA ìë™ ê°ì§€: {torch.cuda.get_device_name(0)}")
            print(f"  GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            device = 'cpu'
            print("âš  CUDA ì—†ìŒ - CPU ëª¨ë“œ")
    elif device == 'cuda' or device.startswith('cuda:'):
        if torch.cuda.is_available():
            print(f"âœ“ CUDA ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            print(f"  GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            device = 'cuda'  # 'cuda:0' â†’ 'cuda'ë¡œ í†µì¼
        else:
            print("âœ— CUDA ë¶ˆê°€ - CPUë¡œ ì „í™˜")
            device = 'cpu'
    elif device == 'cpu':
        device = 'cpu'
        print("âœ“ CPU ëª¨ë“œ ì„ íƒ")
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” device ê°’
        print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” device: {device}, CPUë¡œ ì „í™˜")
        device = 'cpu'
    
    # CUDA ìµœì í™” ì„¤ì •
    if device == 'cuda':
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
        torch.inference_mode().__enter__()
        print("âœ“ CUDA ìµœì í™” í™œì„±í™” (TF32, bfloat16)")
    
    print()
    return device


def load_model(model_dir, device='cuda'):
    """
    SAM3 ëª¨ë¸ ë¡œë“œ (ì˜¤í”„ë¼ì¸ - ë¡œì»¬ ê²½ë¡œ)
    
    Args:
        model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (checkpoint, bpe_vocab í¬í•¨)
        device: 'cuda' ë˜ëŠ” 'cpu'
    """
    print("=" * 60)
    print("ëª¨ë¸ ë¡œë“œ ì¤‘... (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)")
    print("=" * 60)
    
    model_dir = Path(model_dir)
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    checkpoint_path = model_dir / "sam3_checkpoint.pth"
    bpe_path = model_dir / "bpe_simple_vocab_16e6.txt.gz"
    
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
    if not bpe_path.exists():
        raise FileNotFoundError(f"BPE vocabì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bpe_path}")
    
    print(f"âœ“ ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
    print(f"âœ“ BPE Vocab: {bpe_path}")
    
    # ëª¨ë¸ ë¹Œë“œ
    start_time = time.time()
    print("\nëª¨ë¸ êµ¬ì¡° ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # ë¹ˆ ëª¨ë¸ ìƒì„± (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)
        model = build_sam3_image_model(bpe_path=str(bpe_path))
        print("âœ“ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print("âœ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...")
        state_dict = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(state_dict, strict=True)
        print("âœ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        
        load_time = time.time() - start_time
        print(f"âœ“ ì „ì²´ ë¡œë“œ ì‹œê°„: {load_time:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    if device.startswith('cuda'):
        model = model.cuda()
        print("âœ“ ëª¨ë¸ì„ GPUë¡œ ì´ë™")
        
        # CUDA ìºì‹œ ì›Œë°ì—…
        print("âœ“ CUDA ìºì‹œ ì›Œë°ì—… ì¤‘...")
        try:
            if hasattr(model, 'transformer') and hasattr(model.transformer, 'decoder'):
                decoder = model.transformer.decoder
                target_device = torch.device('cuda')
                
                # compilable_cord_cacheë¥¼ CUDAë¡œ ê°•ì œ ì´ë™
                if hasattr(decoder, 'compilable_cord_cache') and decoder.compilable_cord_cache is not None:
                    coords_h, coords_w = decoder.compilable_cord_cache
                    decoder.compilable_cord_cache = (
                        coords_h.to(target_device),
                        coords_w.to(target_device)
                    )
                    print(f"  - compilable_cord_cache â†’ {target_device}")
                
                # coord_cache ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  ì—”íŠ¸ë¦¬ë¥¼ CUDAë¡œ ì´ë™
                if hasattr(decoder, 'coord_cache'):
                    for feat_size, (coords_h, coords_w) in list(decoder.coord_cache.items()):
                        decoder.coord_cache[feat_size] = (
                            coords_h.to(target_device),
                            coords_w.to(target_device)
                        )
                    if decoder.coord_cache:
                        print(f"  - coord_cache ({len(decoder.coord_cache)} entries) â†’ {target_device}")
                
                # Monkey patch _get_rpb_matrix
                original_get_rpb_matrix = decoder._get_rpb_matrix
                
                def patched_get_rpb_matrix(reference_boxes, feat_size):
                    """Patched version that ensures coords are on same device as reference_boxes"""
                    from sam3.model.box_ops import box_cxcywh_to_xyxy
                    
                    H, W = feat_size
                    boxes_xyxy = box_cxcywh_to_xyxy(reference_boxes).transpose(0, 1)
                    bs, num_queries, _ = boxes_xyxy.shape
                    
                    target_dev = reference_boxes.device
                    
                    if decoder.compilable_cord_cache is None:
                        coords_h, coords_w = decoder._get_coords(H, W, target_dev)
                        decoder.compilable_cord_cache = (coords_h, coords_w)
                        decoder.compilable_stored_size = (H, W)
                    
                    if torch.compiler.is_dynamo_compiling() or decoder.compilable_stored_size == (H, W):
                        coords_h, coords_w = decoder.compilable_cord_cache
                        if coords_h.device != target_dev:
                            coords_h = coords_h.to(target_dev)
                            coords_w = coords_w.to(target_dev)
                            decoder.compilable_cord_cache = (coords_h, coords_w)
                    else:
                        if feat_size not in decoder.coord_cache:
                            decoder.coord_cache[feat_size] = decoder._get_coords(H, W, target_dev)
                        coords_h, coords_w = decoder.coord_cache[feat_size]
                        if coords_h.device != target_dev:
                            coords_h = coords_h.to(target_dev)
                            coords_w = coords_w.to(target_dev)
                            decoder.coord_cache[feat_size] = (coords_h, coords_w)
                    
                    deltas_y = coords_h.view(1, -1, 1) - boxes_xyxy.reshape(-1, 1, 4)[:, :, 1:4:2]
                    deltas_y = deltas_y.view(bs, num_queries, -1, 2)
                    deltas_x = coords_w.view(1, -1, 1) - boxes_xyxy.reshape(-1, 1, 4)[:, :, 0:3:2]
                    deltas_x = deltas_x.view(bs, num_queries, -1, 2)
                    
                    if decoder.boxRPB in ["log", "both"]:
                        deltas_x_log = deltas_x * 8
                        deltas_x_log = (
                            torch.sign(deltas_x_log)
                            * torch.log2(torch.abs(deltas_x_log) + 1.0)
                            / np.log2(8)
                        )
                        deltas_y_log = deltas_y * 8
                        deltas_y_log = (
                            torch.sign(deltas_y_log)
                            * torch.log2(torch.abs(deltas_y_log) + 1.0)
                            / np.log2(8)
                        )
                        if decoder.boxRPB == "log":
                            deltas_x = deltas_x_log
                            deltas_y = deltas_y_log
                        else:
                            deltas_x = torch.cat([deltas_x, deltas_x_log], dim=-1)
                            deltas_y = torch.cat([deltas_y, deltas_y_log], dim=-1)
                    
                    deltas_x = decoder.boxRPB_embed_x(deltas_x)
                    deltas_y = decoder.boxRPB_embed_y(deltas_y)
                    
                    B = deltas_y.unsqueeze(3) + deltas_x.unsqueeze(2)
                    B = B.flatten(2, 3)
                    B = B.permute(0, 3, 1, 2)
                    B = B.contiguous()
                    return B
                
                decoder._get_rpb_matrix = patched_get_rpb_matrix
                print(f"  - _get_rpb_matrix patched for device consistency")
                
        except Exception as e:
            print(f"  âš  ìºì‹œ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
    else:
        model = model.cpu()
        print("âœ“ ëª¨ë¸ì„ CPUì— ìœ ì§€")
    
    print()
    return model


def create_transforms():
    """ì „ì²˜ë¦¬ Transform ìƒì„±"""
    return ComposeAPI(
        transforms=[
            RandomResizeAPI(sizes=1008, max_size=1008, square=True, consistent_transform=False),
            ToTensorAPI(),
            NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ]
    )


def create_postprocessor(detection_config=None, class_mapping=None, device='cuda'):
    """
    í›„ì²˜ë¦¬ PostProcessor ìƒì„±

    Args:
        detection_config: detection ì„¤ì • ë”•ì…”ë„ˆë¦¬
            {
                "use_presence": false,
                "default_threshold": 0.3,
                "class_thresholds": {"helmet": 0.15, ...},
                "max_dets_per_img": 100
            }
        class_mapping: í´ë˜ìŠ¤ ì´ë¦„ â†’ ID ë§¤í•‘
        device: 'cuda' or 'cpu'
    """
    # ê¸°ë³¸ê°’ ì„¤ì •
    if detection_config is None:
        detection_config = {
            "use_presence": True,
            "default_threshold": 0.3,
            "max_dets_per_img": 100
        }

    use_presence = detection_config.get('use_presence', True)
    default_threshold = detection_config.get('default_threshold', 0.3)
    class_thresholds = detection_config.get('class_thresholds', {})
    max_dets = detection_config.get('max_dets_per_img', 100)

    # CPU/GPU ì„¤ì •
    to_cpu = (device == 'cpu')
    use_gpu_interpolate = (device != 'cpu')

    # í´ë˜ìŠ¤ë³„ thresholdê°€ ìˆìœ¼ë©´ ClassWise ë²„ì „ ì‚¬ìš©
    if class_thresholds and class_mapping:
        print(f"ğŸ“Š í´ë˜ìŠ¤ë³„ threshold ì‚¬ìš©:")
        print(f"   use_presence: {use_presence}")
        print(f"   default_threshold: {default_threshold}")
        for class_name, threshold in class_thresholds.items():
            print(f"   {class_name}: {threshold}")

        return PostProcessImageWithClassThresholds(
            max_dets_per_img=max_dets,
            class_thresholds=class_thresholds,
            class_to_id=class_mapping,
            use_presence=use_presence,
            detection_threshold=default_threshold,
            iou_type="segm",
            use_original_sizes_box=True,
            use_original_sizes_mask=True,
            convert_mask_to_rle=False,
            to_cpu=to_cpu,
            always_interpolate_masks_on_gpu=use_gpu_interpolate
        )
    else:
        print(f"ğŸ“Š ë‹¨ì¼ threshold ì‚¬ìš©:")
        print(f"   use_presence: {use_presence}")
        print(f"   threshold: {default_threshold}")

        return PostProcessImage(
            max_dets_per_img=max_dets,
            use_presence=use_presence,
            detection_threshold=default_threshold,
            iou_type="segm",
            use_original_sizes_box=True,
            use_original_sizes_mask=True,
            convert_mask_to_rle=False,
            to_cpu=to_cpu,
            always_interpolate_masks_on_gpu=use_gpu_interpolate
        )


def create_datapoint_with_prompts(pil_image, text_prompts):
    """ì´ë¯¸ì§€ì™€ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œ Datapoint ìƒì„±"""
    global GLOBAL_COUNTER
    
    datapoint = Datapoint(find_queries=[], images=[])
    
    w, h = pil_image.size
    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h, w])]
    
    prompt_ids = []
    for text_query in text_prompts:
        datapoint.find_queries.append(
            FindQueryLoaded(
                query_text=text_query,
                image_id=0,
                object_ids_output=[],
                is_exhaustive=True,
                query_processing_order=0,
                inference_metadata=InferenceMetadata(
                    coco_image_id=GLOBAL_COUNTER,
                    original_image_id=GLOBAL_COUNTER,
                    original_category_id=1,
                    original_size=[h, w],
                    object_id=0,
                    frame_index=0,
                )
            )
        )
        prompt_ids.append(GLOBAL_COUNTER)
        GLOBAL_COUNTER += 1
    
    return datapoint, prompt_ids


def parse_image_source(image_source):
    """ì´ë¯¸ì§€ ì†ŒìŠ¤ íŒŒì‹±"""
    image_paths = []
    
    if os.path.isdir(image_source):
        print(f"ğŸ“ í´ë”ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰: {image_source}")
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        
        image_paths_set = set()
        for ext in extensions:
            image_paths_set.update(Path(image_source).glob(f"*{ext}"))
            image_paths_set.update(Path(image_source).glob(f"*{ext.upper()}"))
        
        image_paths = [str(p) for p in sorted(image_paths_set)]
        print(f"  âœ“ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    elif os.path.isfile(image_source):
        print(f"ğŸ“„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ: {image_source}")
        
        with open(image_source, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and os.path.exists(line):
                    image_paths.append(line)
        
        print(f"  âœ“ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ")
    
    else:
        raise ValueError(f"ì˜ëª»ëœ ì†ŒìŠ¤: {image_source}")
    
    if len(image_paths) == 0:
        raise ValueError("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return sorted(image_paths)


def parse_video_source(video_source):
    """ë™ì˜ìƒ ì†ŒìŠ¤ íŒŒì‹±"""
    video_paths = []
    
    if os.path.isdir(video_source):
        print(f"ğŸ“ í´ë”ì—ì„œ ë™ì˜ìƒ ê²€ìƒ‰: {video_source}")
        extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
        
        video_paths_set = set()
        for ext in extensions:
            video_paths_set.update(Path(video_source).glob(f"*{ext}"))
            video_paths_set.update(Path(video_source).glob(f"*{ext.upper()}"))
        
        video_paths = [str(p) for p in sorted(video_paths_set)]
        print(f"  âœ“ {len(video_paths)}ê°œ ë™ì˜ìƒ ë°œê²¬")
    
    elif os.path.isfile(video_source):
        print(f"ğŸ“„ ë™ì˜ìƒ íŒŒì¼: {video_source}")
        video_paths = [video_source]
    
    else:
        raise ValueError(f"ì˜ëª»ëœ ë™ì˜ìƒ ì†ŒìŠ¤: {video_source}")
    
    if len(video_paths) == 0:
        raise ValueError("ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return sorted(video_paths)


def parse_class_mapping(class_str):
    """í´ë˜ìŠ¤ ë§¤í•‘ íŒŒì‹±"""
    if isinstance(class_str, dict):
        return class_str
    
    mapping = {}
    pairs = class_str.split(',')
    
    for pair in pairs:
        pair = pair.strip()
        if ':' in pair:
            name, idx = pair.split(':')
            mapping[name.strip()] = int(idx.strip())
    
    return mapping


def bbox_to_yolo_format(box, img_width, img_height):
    """ë°•ìŠ¤ë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    x1, y1, x2, y2 = box
    
    x_center = (x1 + x2) / 2.0 / img_width
    y_center = (y1 + y2) / 2.0 / img_height
    width = (x2 - x1) / img_width
    height = (y2 - y1) / img_height
    
    return x_center, y_center, width, height


def save_yolo_annotation(image_path, results_by_prompt, class_mapping, output_dir, img_width, img_height):
    """YOLO í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ì €ì¥"""
    image_name = Path(image_path).stem
    txt_path = os.path.join(output_dir, f"{image_name}.txt")
    
    lines = []
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        class_id = class_mapping.get(prompt_name, -1)
        if class_id < 0:
            continue
        
        boxes = result['boxes']
        scores = result['scores']
        
        for idx, (box, score) in enumerate(zip(boxes, scores)):
            x_center, y_center, width, height = bbox_to_yolo_format(
                box, img_width, img_height
            )
            
            line = f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"
            lines.append(line)
            total_objects += 1
    
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    
    return total_objects


def imread_unicode(image_path):
    """í•œê¸€ ê²½ë¡œ ì§€ì› ì´ë¯¸ì§€ ì½ê¸°"""
    import cv2
    import numpy as np
    
    try:
        stream = open(image_path, "rb")
        bytes_data = bytearray(stream.read())
        numpy_array = np.asarray(bytes_data, dtype=np.uint8)
        image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
        stream.close()
        return image
    except:
        return None


def imwrite_unicode(image_path, image):
    """í•œê¸€ ê²½ë¡œ ì§€ì› ì´ë¯¸ì§€ ì €ì¥"""
    import cv2
    
    try:
        ext = os.path.splitext(image_path)[1]
        result, encoded_img = cv2.imencode(ext, image)
        if result:
            with open(image_path, mode='w+b') as f:
                encoded_img.tofile(f)
            return True
        return False
    except:
        return False


def save_visualization_result(image_path, results_by_prompt, class_mapping, output_dir):
    """ì‹œê°í™” ê²°ê³¼ ì €ì¥"""
    import cv2
    
    image = imread_unicode(image_path)
    if image is None:
        return None
    
    colors = [
        (0, 255, 0), (255, 0, 0), (0, 0, 255),
        (0, 255, 255), (255, 0, 255), (255, 255, 0),
    ]
    
    color_idx = 0
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        color = colors[color_idx % len(colors)]
        color_idx += 1
        
        boxes = result['boxes']
        scores = result['scores']
        
        for box, score in zip(boxes, scores):
            x1, y1, x2, y2 = [int(v) for v in box]
            
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            
            label = f"{prompt_name}: {score:.2f}"
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            cv2.rectangle(
                image,
                (x1, y1 - text_height - 10),
                (x1 + text_width, y1),
                (0, 0, 0),
                -1
            )
            
            cv2.putText(
                image,
                label,
                (x1, y1 - 5),
                font,
                font_scale,
                (255, 255, 255),
                thickness
            )
            
            total_objects += 1
    
    info_text = f"{Path(image_path).name} | Objects: {total_objects}"
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    os.makedirs(output_dir, exist_ok=True)
    image_name = Path(image_path).stem
    save_path = os.path.join(output_dir, f"{image_name}_result.jpg")
    imwrite_unicode(save_path, image)
    
    return save_path


def show_realtime_result(image_path, results_by_prompt, class_mapping, window_name="SAM3 Detection"):
    """ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ"""
    import cv2
    
    image = imread_unicode(image_path)
    if image is None:
        return
    
    colors = [
        (0, 255, 0), (255, 0, 0), (0, 0, 255),
        (0, 255, 255), (255, 0, 255), (255, 255, 0),
    ]
    
    color_idx = 0
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        color = colors[color_idx % len(colors)]
        color_idx += 1
        
        boxes = result['boxes']
        scores = result['scores']
        
        for box, score in zip(boxes, scores):
            x1, y1, x2, y2 = [int(v) for v in box]
            
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            
            label = f"{prompt_name}: {score:.2f}"
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            cv2.rectangle(
                image,
                (x1, y1 - text_height - 10),
                (x1 + text_width, y1),
                (0, 0, 0),
                -1
            )
            
            cv2.putText(
                image,
                label,
                (x1, y1 - 5),
                font,
                font_scale,
                (255, 255, 255),
                thickness
            )
            
            total_objects += 1
    
    info_text = f"{Path(image_path).name} | Objects: {total_objects}"
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    cv2.resizeWindow(window_name, 1280, 720)
    cv2.imshow(window_name, image)
    cv2.waitKey(200)


def extract_frames_from_videos(video_source, jpeg_output_dir, fps_extraction=1, verbose=True):
    """ë™ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ

    Args:
        video_source: ë™ì˜ìƒ íŒŒì¼/í´ë” ê²½ë¡œ
        jpeg_output_dir: JPEGImages ì €ì¥ ê²½ë¡œ
        fps_extraction: Ní”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ ì¶”ì¶œ (1=ë§¤ í”„ë ˆì„, 30=30í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ, 0/-1=ì›ë³¸ ì „ì²´)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    """
    import cv2

    print("\n" + "=" * 60)
    print("ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘")
    print("=" * 60)

    video_paths = parse_video_source(video_source)

    os.makedirs(jpeg_output_dir, exist_ok=True)
    print(f"ğŸ“ JPEGImages ì €ì¥ ê²½ë¡œ: {jpeg_output_dir}\n")

    total_extracted = 0
    global_frame_index = 1

    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False

    for video_idx, video_path in enumerate(video_paths):
        video_name = Path(video_path).stem

        if verbose:
            print(f"\n[{video_idx+1}/{len(video_paths)}] {video_name}")

        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"  âœ— ë™ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            continue

        original_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / original_fps if original_fps > 0 else 0

        if verbose:
            print(f"  ì›ë³¸ FPS: {original_fps:.2f}")
            print(f"  ì´ í”„ë ˆì„: {total_frames}")
            print(f"  ê¸¸ì´: {duration:.2f}ì´ˆ")

        if fps_extraction <= 0:
            # 0 ë˜ëŠ” ìŒìˆ˜ë©´ ì›ë³¸ ì „ì²´ ì¶”ì¶œ
            frame_interval = 1
            if verbose:
                print(f"  ì¶”ì¶œ ëª¨ë“œ: ì›ë³¸ ì „ì²´ (ë§¤ 1í”„ë ˆì„)")
        else:
            # fps_extraction = Ní”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ ì¶”ì¶œ
            frame_interval = fps_extraction
            if verbose:
                print(f"  ì¶”ì¶œ ëª¨ë“œ: ë§¤ {frame_interval}í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ")
                actual_fps = original_fps / frame_interval if frame_interval > 0 else original_fps
                print(f"  ì‹¤ì œ ì¶”ì¶œ FPS: {actual_fps:.2f}fps")
        
        estimated_frames = total_frames // frame_interval
        if verbose:
            print(f"  ì˜ˆìƒ ì¶”ì¶œ: {estimated_frames}í”„ë ˆì„\n")
        
        frame_count = 0
        extracted_count = 0
        
        iterator = tqdm(total=total_frames, desc=f"  {video_name}") if use_tqdm else range(total_frames)
        
        while True:
            ret, frame = cap.read()
            
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                frame_filename = f"{video_name}_frame_{global_frame_index:06d}.jpg"
                frame_path = os.path.join(jpeg_output_dir, frame_filename)
                
                success = imwrite_unicode(frame_path, frame)
                
                if success:
                    extracted_count += 1
                    global_frame_index += 1
            
            frame_count += 1
            
            if use_tqdm:
                iterator.update(1)
        
        if use_tqdm:
            iterator.close()
        
        cap.release()
        
        if verbose:
            print(f"  âœ“ ì¶”ì¶œ ì™„ë£Œ: {extracted_count}í”„ë ˆì„")
        
        total_extracted += extracted_count
    
    print("\n" + "=" * 60)
    print("í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ!")
    print("=" * 60)
    print(f"âœ“ ì²˜ë¦¬ ë™ì˜ìƒ: {len(video_paths)}ê°œ")
    print(f"âœ“ ì¶”ì¶œ í”„ë ˆì„: {total_extracted}ê°œ")
    print(f"âœ“ ì €ì¥ ê²½ë¡œ: {jpeg_output_dir}")
    print("=" * 60 + "\n")
    
    return total_extracted


def process_single_image_batch(
    image_path, model, transform, postprocessor, prompts, 
    class_mapping, output_dir, device='cuda',
    show_realtime=False, save_visualizations=False, 
    visualization_dir=None, window_name="SAM3 Detection",
    prompt_chunk_size=4
):
    """ë‹¨ì¼ ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        total_start = time.time()
        
        load_start = time.time()
        pil_image = Image.open(image_path)

        if pil_image.mode != "RGB":
            pil_image = pil_image.convert("RGB")

        pil_image = Image.fromarray(np.array(pil_image))

        img_width, img_height = pil_image.size
        load_time = time.time() - load_start
        
        results_by_prompt = {}
        
        total_prep_time = 0
        total_inference_time = 0
        total_post_time = 0
        
        num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * prompt_chunk_size
            end_idx = min(start_idx + prompt_chunk_size, len(prompts))
            chunk_prompts = prompts[start_idx:end_idx]
            
            prep_start = time.time()
            datapoint, prompt_ids = create_datapoint_with_prompts(pil_image, chunk_prompts)
            
            datapoint = transform(datapoint)
            
            batch = collate([datapoint], dict_key="dummy")["dummy"]
            
            target_device = torch.device(device if device.startswith('cuda') else 'cpu')
            
            batch = copy_data_to_device(batch, target_device, non_blocking=True)
            
            batch = recursive_to_device(batch, target_device)
            
            prep_time = time.time() - prep_start
            total_prep_time += prep_time
            
            if device.startswith('cuda'):
                torch.cuda.synchronize()
            
            inference_start = time.time()
            output = model(batch)
            
            if device.startswith('cuda'):
                torch.cuda.synchronize()
            inference_time = time.time() - inference_start
            total_inference_time += inference_time
            
            post_start = time.time()
            processed_results = postprocessor.process_results(output, batch.find_metadatas)
            post_time = time.time() - post_start
            total_post_time += post_time
            
            if not isinstance(processed_results, list):
                if isinstance(processed_results, dict):
                    for prompt_name, prompt_id in zip(chunk_prompts, prompt_ids):
                        if prompt_id in processed_results:
                            result = processed_results[prompt_id]
                            boxes = result['boxes'].float().cpu().numpy() if hasattr(result['boxes'], 'cpu') else result['boxes']
                            scores = result['scores'].float().cpu().numpy() if hasattr(result['scores'], 'cpu') else result['scores']
                            
                            results_by_prompt[prompt_name] = {
                                'boxes': boxes,
                                'scores': scores
                            }
                        else:
                            if prompt_name not in results_by_prompt:
                                results_by_prompt[prompt_name] = {
                                    'boxes': np.array([]),
                                    'scores': np.array([])
                                }
                else:
                    for prompt_name in chunk_prompts:
                        if prompt_name not in results_by_prompt:
                            results_by_prompt[prompt_name] = {
                                'boxes': np.array([]),
                                'scores': np.array([])
                            }
            else:
                for result in processed_results:
                    if isinstance(result, dict) and 'query_id' in result:
                        query_id = result['query_id']
                        
                        for prompt_name, prompt_id in zip(chunk_prompts, prompt_ids):
                            if query_id == prompt_id:
                                boxes = result['boxes'].float().cpu().numpy() if hasattr(result['boxes'], 'cpu') else result['boxes']
                                scores = result['scores'].float().cpu().numpy() if hasattr(result['scores'], 'cpu') else result['scores']
                                results_by_prompt[prompt_name] = {
                                    'boxes': boxes,
                                    'scores': scores
                                }
                                break
                
                for prompt_name in chunk_prompts:
                    if prompt_name not in results_by_prompt:
                        results_by_prompt[prompt_name] = {
                            'boxes': np.array([]),
                            'scores': np.array([])
                        }
            
            if device.startswith('cuda'):
                del batch, output, processed_results
                torch.cuda.empty_cache()
        
        save_start = time.time()
        num_objects = save_yolo_annotation(
            image_path, results_by_prompt, class_mapping, 
            output_dir, img_width, img_height
        )
        save_time = time.time() - save_start
        
        if show_realtime:
            show_realtime_result(image_path, results_by_prompt, class_mapping, window_name)
        
        visualization_path = None
        if save_visualizations and visualization_dir:
            visualization_path = save_visualization_result(
                image_path, results_by_prompt, class_mapping, visualization_dir
            )
        
        total_time = time.time() - total_start
        
        return {
            'success': True,
            'num_objects': num_objects,
            'image_size': (img_width, img_height),
            'visualization_path': visualization_path,
            'timing': {
                'total': total_time,
                'load': load_time,
                'preprocess': total_prep_time,
                'inference': total_inference_time,
                'postprocess': total_post_time,
                'save': save_time
            },
            'num_chunks': num_chunks
        }
        
    except Exception as e:
        import traceback
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {image_path}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'num_objects': 0,
            'visualization_path': None
        }


def create_yolo_dataset(
    image_source,
    output_dir,
    class_mapping,
    model_dir,
    prompts=None,
    model=None,
    transform=None,
    postprocessor=None,
    detection_threshold=0.3,
    device='cuda',
    prompt_chunk_size=4,
    verbose=True,
    show_realtime=False,
    save_visualizations=False,
    visualization_dir=None
):
    """YOLO í˜•ì‹ ë°ì´í„°ì…‹ ìƒì„±"""
    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 12 + "YOLO Dataset Creation Tool" + " " * 12 + "â•‘")
    print("â•‘" + " " * 15 + "(Batch Inference Mode)" + " " * 15 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()
    
    start_time = time.time()
    
    print("=" * 60)
    print("ì„¤ì • í™•ì¸")
    print("=" * 60)
    
    class_mapping = parse_class_mapping(class_mapping)
    print(f"í´ë˜ìŠ¤ ë§¤í•‘: {class_mapping}")
    
    if prompts is None:
        prompts = list(class_mapping.keys())
    print(f"í”„ë¡¬í”„íŠ¸: {prompts}")
    print(f"í”„ë¡¬í”„íŠ¸ ì´ ê°œìˆ˜: {len(prompts)}ê°œ")
    print(f"í”„ë¡¬í”„íŠ¸ ì²­í¬ í¬ê¸°: {prompt_chunk_size}ê°œ")
    print(f"ì²­í¬ ìˆ˜: {(len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size}ê°œ")
    print(f"ê²€ì¶œ ì„ê³„ê°’: {detection_threshold}")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì‹¤ì‹œê°„ í‘œì‹œ: {show_realtime}")
    print(f"ì‹œê°í™” ì €ì¥: {save_visualizations}")
    if save_visualizations and visualization_dir:
        print(f"ì‹œê°í™” ë””ë ‰í† ë¦¬: {visualization_dir}")
    print()
    
    print("=" * 60)
    print("ì´ë¯¸ì§€ ë¡œë“œ")
    print("=" * 60)
    image_paths = parse_image_source(image_source)
    print()
    
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ ë¼ë²¨ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    if save_visualizations and visualization_dir:
        os.makedirs(visualization_dir, exist_ok=True)
        print(f"ğŸ“ ì‹œê°í™” ì¶œë ¥ ë””ë ‰í† ë¦¬: {visualization_dir}")
    print()
    
    if model is None:
        model = load_model(model_dir, device)
    
    if transform is None:
        transform = create_transforms()
        print("âœ“ Transform ìƒì„± ì™„ë£Œ")
    
    if postprocessor is None:
        # detection_config ê¸°ë³¸ê°’ ìƒì„±
        default_detection_config = {
            "use_presence": True,
            "default_threshold": detection_threshold,
            "max_dets_per_img": 100
        }
        postprocessor = create_postprocessor(
            detection_config=default_detection_config,
            class_mapping=class_mapping,
            device=device
        )
        print("âœ“ PostProcessor ìƒì„± ì™„ë£Œ")
    
    print()
    
    window_name = "SAM3 Real-time Detection"
    if show_realtime:
        import cv2
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        print("ğŸ–¥ï¸  ì‹¤ì‹œê°„ í‘œì‹œ ìœˆë„ìš° ìƒì„±\n")
    
    print("=" * 60)
    print("ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ ì¶”ë¡  ëª¨ë“œ + ì²­í¬ ì²˜ë¦¬)")
    print("=" * 60)
    num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
    print(f"âœ“ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œë¥¼ {num_chunks}ê°œ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬")
    print(f"âœ“ ì²­í¬ë‹¹ {prompt_chunk_size}ê°œ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬")
    print(f"âœ“ ì´ë¯¸ì§€ë‹¹ ì´ {num_chunks}ë²ˆ forward\n")
    
    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False
    
    total_objects = 0
    success_count = 0
    fail_count = 0
    
    timing_stats = {
        'total': [],
        'load': [],
        'preprocess': [],
        'inference': [],
        'postprocess': [],
        'save': []
    }
    
    first_inference_time = None
    
    iterator = tqdm(image_paths, desc="Processing") if use_tqdm else image_paths
    
    for idx, image_path in enumerate(iterator):
        if not use_tqdm and verbose:
            print(f"[{idx+1}/{len(image_paths)}] {Path(image_path).name}", end=" ... ")
        
        result = process_single_image_batch(
            image_path, model, transform, postprocessor, prompts,
            class_mapping, output_dir, device,
            show_realtime=show_realtime,
            save_visualizations=save_visualizations,
            visualization_dir=visualization_dir,
            window_name=window_name,
            prompt_chunk_size=prompt_chunk_size
        )
        
        if result['success']:
            success_count += 1
            total_objects += result['num_objects']
            
            timing = result.get('timing', {})
            for key in timing_stats.keys():
                if key in timing:
                    timing_stats[key].append(timing[key])
            
            if first_inference_time is None and 'inference' in timing:
                first_inference_time = timing['inference']
            
            if not use_tqdm and verbose:
                print(f"âœ“ ({result['num_objects']} objects, {timing.get('inference', 0):.3f}s)")
        else:
            fail_count += 1
            if not use_tqdm and verbose:
                print(f"âœ— {result['error']}")
    
    if show_realtime:
        import cv2
        cv2.destroyAllWindows()
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ì™„ë£Œ!")
    print("=" * 60)
    print(f"âœ“ ì„±ê³µ: {success_count}/{len(image_paths)} ì´ë¯¸ì§€")
    print(f"âœ“ ì´ ê°ì²´ ìˆ˜: {total_objects}")
    print(f"âœ“ í‰ê·  ê°ì²´/ì´ë¯¸ì§€: {total_objects/max(success_count,1):.1f}")
    print(f"âœ“ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}ê°œ")
    num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
    print(f"âœ“ ì²­í¬ ì²˜ë¦¬ ëª¨ë“œ: ì´ë¯¸ì§€ë‹¹ {num_chunks}ë²ˆ forward ({prompt_chunk_size}ê°œì”©)")
    
    if fail_count > 0:
        print(f"âœ— ì‹¤íŒ¨: {fail_count} ì´ë¯¸ì§€")
    
    print(f"\nâ±  ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"â±  ì´ë¯¸ì§€ë‹¹ í‰ê· : {elapsed_time/len(image_paths):.2f}ì´ˆ")
    
    if timing_stats['inference']:
        print("\n" + "=" * 60)
        print("ğŸ“Š ìƒì„¸ íƒ€ì´ë° í†µê³„ (í‰ê· )")
        print("=" * 60)
        
        if first_inference_time is not None:
            print(f"ì²« ì¶”ë¡  ì‹œê°„ (ì»´íŒŒì¼ í¬í•¨): {first_inference_time:.3f}ì´ˆ")
        
        if len(timing_stats['inference']) > 1:
            avg_inference_without_compile = np.mean(timing_stats['inference'][1:])
            print(f"ì´í›„ ì¶”ë¡  í‰ê·  (ì»´íŒŒì¼ ì œì™¸): {avg_inference_without_compile:.3f}ì´ˆ")
        
        print(f"\nê° ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„:")
        print(f"  - ì´ë¯¸ì§€ ë¡œë“œ:     {np.mean(timing_stats['load']):.3f}ì´ˆ")
        print(f"  - ì „ì²˜ë¦¬:          {np.mean(timing_stats['preprocess']):.3f}ì´ˆ")
        print(f"  - ì¶”ë¡  (forward):  {np.mean(timing_stats['inference']):.3f}ì´ˆ")
        print(f"  - í›„ì²˜ë¦¬:          {np.mean(timing_stats['postprocess']):.3f}ì´ˆ")
        print(f"  - ì €ì¥:            {np.mean(timing_stats['save']):.3f}ì´ˆ")
        print(f"  - ì „ì²´:            {np.mean(timing_stats['total']):.3f}ì´ˆ")
        
        print(f"\nì¶”ë¡  ì„±ëŠ¥ ë¶„ì„:")
        inference_times = timing_stats['inference']
        print(f"  - ìµœì†Œ: {np.min(inference_times):.3f}ì´ˆ")
        print(f"  - ìµœëŒ€: {np.max(inference_times):.3f}ì´ˆ")
        print(f"  - í‰ê· : {np.mean(inference_times):.3f}ì´ˆ")
        print(f"  - ì¤‘ì•™ê°’: {np.median(inference_times):.3f}ì´ˆ")
        
        print(f"\në””ë°”ì´ìŠ¤: {device.upper()}")
        if device.startswith('cuda'):
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
        
        avg_inference = np.mean(inference_times)
        time_per_prompt = avg_inference / len(prompts)
        print(f"\níš¨ìœ¨ì„±:")
        print(f"  - í”„ë¡¬í”„íŠ¸ë‹¹ í‰ê·  ì‹œê°„: {time_per_prompt:.3f}ì´ˆ")
        print(f"  - ì´ˆë‹¹ ì²˜ë¦¬ í”„ë¡¬í”„íŠ¸: {1/time_per_prompt:.1f}ê°œ")
    
    print(f"\nğŸ“ ë¼ë²¨ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    if save_visualizations and visualization_dir:
        print(f"ğŸ“ ì‹œê°í™” ì €ì¥ ìœ„ì¹˜: {visualization_dir}")
    print("=" * 60)


def load_config_from_json(config_path):
    """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="SAM3 YOLO Dataset Creator (Offline Server)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # GPU 0ë²ˆì—ì„œ ì‹¤í–‰
  python sam3_offline.py --gpu 0 --model_dir ./models --image_dir ./data/images

  # Config íŒŒì¼ ì‚¬ìš©
  python sam3_offline.py --config config.json --gpu 1

  # ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ + ë¼ë²¨ ìƒì„±
  python sam3_offline.py --gpu 0 --video_source ./videos --fps 1
        """
    )
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument('--gpu', type=int, default=None,
                        help='GPU ì¸ë±ìŠ¤ (0, 1, 2, ...) - Noneì´ë©´ ìë™')
    parser.add_argument('--model_dir', type=str, default='./models',
                        help='ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--config', type=str, default=None,
                        help='JSON ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    # ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ
    parser.add_argument('--video_source', type=str, default=None,
                        help='ë™ì˜ìƒ íŒŒì¼/í´ë” ê²½ë¡œ')
    parser.add_argument('--fps', type=int, default=1,
                        help='Ní”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ ì¶”ì¶œ (1=ë§¤í”„ë ˆì„, 30=30í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ, 0/-1=ì›ë³¸ ì „ì²´)')
    
    # ì´ë¯¸ì§€ ì†ŒìŠ¤
    parser.add_argument('--image_dir', type=str, default=None,
                        help='ì´ë¯¸ì§€ í´ë” ê²½ë¡œ')
    parser.add_argument('--jpeg_dir', type=str, default='./data/JPEGImages',
                        help='JPEGImages ê²½ë¡œ (ë™ì˜ìƒ ì¶”ì¶œìš©)')
    
    # ì¶œë ¥ ê²½ë¡œ
    parser.add_argument('--label_dir', type=str, default=None,
                        help='ë¼ë²¨ ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸ê°’: ./data/labels)')
    parser.add_argument('--viz_dir', type=str, default=None,
                        help='ì‹œê°í™” ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸ê°’: ./data/results)')
    
    # í´ë˜ìŠ¤ ì„¤ì •
    parser.add_argument('--classes', type=str, default=None,
                        help='í´ë˜ìŠ¤ ë§¤í•‘ (ì˜ˆ: "person:0,car:1,dog:2")')
    
    # ì¶”ë¡  ì„¤ì •
    parser.add_argument('--threshold', type=float, default=0.3,
                        help='ê²€ì¶œ ì„ê³„ê°’')
    parser.add_argument('--chunk_size', type=int, default=4,
                        help='í”„ë¡¬í”„íŠ¸ ì²­í¬ í¬ê¸°')
    
    # í‘œì‹œ ì˜µì…˜
    parser.add_argument('--show', action='store_true',
                        help='ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ')
    parser.add_argument('--save_viz', action='store_true',
                        help='ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥')
    
    args = parser.parse_args()
    
    # Config íŒŒì¼ ìš°ì„  ë¡œë“œ
    detection_config = None
    if args.config:
        print(f"ğŸ“„ Config íŒŒì¼ ë¡œë“œ: {args.config}")
        config = load_config_from_json(args.config)

        # detection_config ì¶”ì¶œ
        detection_config = config.get('detection_config', None)

        # video_config ì¶”ì¶œ ë° ì ìš©
        video_config = config.get('video_config', None)
        if video_config:
            print("ğŸ“¹ video_config ë°œê²¬:")
            print(f"  {video_config}")
            print("ğŸ“¹ video_config ì ìš© ì¤‘...")

            if args.video_source is None:
                args.video_source = video_config.get('video_source', None)
                if args.video_source:
                    print(f"  âœ“ video_source: {args.video_source}")

            if args.fps == 1:  # ê¸°ë³¸ê°’ì´ë©´
                config_fps = video_config.get('fps', None)
                if config_fps is not None:
                    args.fps = config_fps
                    print(f"  âœ“ fps: {args.fps}")

            if args.jpeg_dir == './data/JPEGImages':  # ê¸°ë³¸ê°’ì´ë©´
                config_jpeg_dir = video_config.get('jpeg_dir', None)
                if config_jpeg_dir:
                    args.jpeg_dir = config_jpeg_dir
                    print(f"  âœ“ jpeg_dir: {args.jpeg_dir}")

        # Config ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸° (ì»¤ë§¨ë“œë¼ì¸ ì¸ìê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        for key, value in config.items():
            if key in ['detection_config', 'video_config', 'inference', 'output']:
                continue  # íŠ¹ìˆ˜ configëŠ” ë³„ë„ ì²˜ë¦¬
            if not hasattr(args, key) or getattr(args, key) is None:
                setattr(args, key, value)

        # inference config ì ìš©
        inference_config = config.get('inference', {})
        if 'chunk_size' in inference_config and args.chunk_size == 4:  # ê¸°ë³¸ê°’ì´ë©´
            args.chunk_size = inference_config['chunk_size']

        # output config ì ìš©
        output_config = config.get('output', {})
        if 'save_viz' in output_config and not args.save_viz:  # ê¸°ë³¸ê°’ì´ë©´
            args.save_viz = output_config['save_viz']
        if 'show' in output_config and not args.show:  # ê¸°ë³¸ê°’ì´ë©´
            args.show = output_config['show']
    
    # ê¸°ë³¸ ì„¤ì •
    if args.classes is None:
        args.classes = {
            "person": 0,
            "car": 1,
            "bicycle": 2
        }
        print("âš  í´ë˜ìŠ¤ ë§¤í•‘ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©:")
        print(f"   {args.classes}")

    if args.label_dir is None:
        args.label_dir = './data/labels'

    if args.viz_dir is None:
        args.viz_dir = './data/results'

    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 5 + "SAM3 YOLO Dataset Creator (Offline Server)" + " " * 5 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()
    
    try:
        # í™˜ê²½ ì„¤ì •
        device = setup_environment(gpu_id=args.gpu, device='auto')
        
        # 1ë‹¨ê³„: ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ
        if args.video_source is not None:
            print("\n" + "ğŸ¬ " * 20)
            print("1ë‹¨ê³„: ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ")
            print("ğŸ¬ " * 20 + "\n")
            print(f"ğŸ“Š í”„ë ˆì„ ì¶”ì¶œ ì„¤ì •:")
            print(f"  - ë™ì˜ìƒ ì†ŒìŠ¤: {args.video_source}")
            print(f"  - ì¶œë ¥ ê²½ë¡œ: {args.jpeg_dir}")
            print(f"  - FPS: {args.fps}\n")

            extract_frames_from_videos(
                video_source=args.video_source,
                jpeg_output_dir=args.jpeg_dir,
                fps_extraction=args.fps,
                verbose=True
            )
            
            print("\n" + "âœ“ " * 20)
            print("1ë‹¨ê³„ ì™„ë£Œ: í”„ë ˆì„ ì¶”ì¶œ ì„±ê³µ!")
            print("âœ“ " * 20 + "\n")
            
            # image_dir ìë™ ì„¤ì •
            if args.image_dir is None:
                args.image_dir = args.jpeg_dir
        
        # 2ë‹¨ê³„: YOLO ë¼ë²¨ ìƒì„±
        if args.image_dir is None:
            print("âœ— ì´ë¯¸ì§€ ì†ŒìŠ¤ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   --image_dir ë˜ëŠ” --video_sourceë¥¼ ì§€ì •í•˜ì„¸ìš”.")
            sys.exit(1)
        
        print("\n" + "ğŸ·ï¸ " * 20)
        print("2ë‹¨ê³„: YOLO ë¼ë²¨ ìƒì„±")
        print("ğŸ·ï¸ " * 20 + "\n")
        
        # ëª¨ë¸ ë¡œë“œ
        model = load_model(args.model_dir, device)
        
        # Transform & PostProcessor ìƒì„±
        transform = create_transforms()

        # detection_configê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
        if detection_config is None:
            detection_config = {
                "use_presence": True,
                "default_threshold": args.threshold,
                "max_dets_per_img": 100
            }

        postprocessor = create_postprocessor(
            detection_config=detection_config,
            class_mapping=args.classes,
            device=device
        )
        
        # YOLO ë°ì´í„°ì…‹ ìƒì„±
        create_yolo_dataset(
            image_source=args.image_dir,
            output_dir=args.label_dir,
            class_mapping=args.classes,
            model_dir=args.model_dir,
            prompts=None,
            model=model,
            transform=transform,
            postprocessor=postprocessor,
            detection_threshold=args.threshold,
            device=device,
            prompt_chunk_size=args.chunk_size,
            verbose=True,
            show_realtime=args.show,
            save_visualizations=args.save_viz,
            visualization_dir=args.viz_dir if args.save_viz else None
        )
        
        print("\n" + "=" * 60)
        print("âœ“ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)
        
        # ìµœì¢… ìš”ì•½
        print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
        if args.video_source is not None:
            print(f"  1. ë™ì˜ìƒ ì†ŒìŠ¤: {args.video_source}")
            print(f"  2. ì¶”ì¶œ FPS: {args.fps}")
            print(f"  3. JPEGImages: {args.jpeg_dir}")
        else:
            print(f"  1. ì´ë¯¸ì§€ ì†ŒìŠ¤: {args.image_dir}")
        print(f"  2. YOLO ë¼ë²¨: {args.label_dir}")
        if args.save_viz:
            print(f"  3. ì‹œê°í™”: {args.viz_dir}")
        print(f"  4. GPU: {args.gpu if args.gpu is not None else 'auto'}")
        
    except KeyboardInterrupt:
        print("\n\nâš  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()