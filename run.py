"""
SAM 3 YOLO Dataset Creator - Video Frame Extraction Support
ë™ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ í›„ YOLO ë°ì´í„°ì…‹ ìƒì„±
"""

import os
import time
import torch
import numpy as np
from PIL import Image
from pathlib import Path

os.environ["CUDA_PATH"] = "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6"

import certifi
os.environ['SSL_CERT_FILE'] = certifi.where()

# SAM3 imports
from sam3 import build_sam3_image_model
from sam3.train.data.collator import collate_fn_api as collate
from sam3.model.utils.misc import copy_data_to_device
from sam3.train.data.sam3_image_dataset import (
    InferenceMetadata, FindQueryLoaded, 
    Image as SAMImage, Datapoint
)
from sam3.train.transforms.basic_for_api import (
    ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI
)
from sam3.eval.postprocessors import PostProcessImage

# Global counter for query IDs
GLOBAL_COUNTER = 1


def recursive_to_device(obj, device):
    """
    ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í…ì„œë¥¼ deviceë¡œ ì´ë™
    """
    if isinstance(obj, torch.Tensor):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: recursive_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [recursive_to_device(item, device) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(recursive_to_device(item, device) for item in obj)
    elif hasattr(obj, '__dict__'):
        # ê°ì²´ì˜ ëª¨ë“  ì†ì„±ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
        for attr_name in dir(obj):
            if not attr_name.startswith('_'):
                try:
                    attr = getattr(obj, attr_name)
                    if isinstance(attr, (torch.Tensor, dict, list, tuple)) or hasattr(attr, '__dict__'):
                        setattr(obj, attr_name, recursive_to_device(attr, device))
                except (AttributeError, TypeError):
                    pass
        return obj
    else:
        return obj


def setup_environment(hf_token=None, device='auto'):
    """í™˜ê²½ ì„¤ì •"""
    print("=" * 60)
    print("í™˜ê²½ ì„¤ì • ì¤‘...")
    print("=" * 60)
    
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
        print("âœ“ HuggingFace í† í° ì„¤ì • ì™„ë£Œ")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"âœ“ CUDA ìë™ ê°ì§€: {torch.cuda.get_device_name(0)}")
            print(f"  GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            device = 'cpu'
            print("âš  CUDA ì—†ìŒ - CPU ëª¨ë“œ")
    elif device == 'cuda':
        if torch.cuda.is_available():
            print(f"âœ“ CUDA ê°•ì œ ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
        else:
            print("âœ— CUDA ë¶ˆê°€ - CPUë¡œ ì „í™˜")
            device = 'cpu'
    else:
        device = 'cpu'
        print("âœ“ CPU ëª¨ë“œ ì„ íƒ")
    
    # CUDA ìµœì í™” ì„¤ì •
    if device == 'cuda':
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
        torch.inference_mode().__enter__()
        print("âœ“ CUDA ìµœì í™” í™œì„±í™” (TF32, bfloat16)")
    
    print()
    return device


def load_model(bpe_path, device='cuda'):
    """SAM3 ëª¨ë¸ ë¡œë“œ"""
    print("=" * 60)
    print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print("=" * 60)
    
    start_time = time.time()
    model = build_sam3_image_model(bpe_path=bpe_path)
    load_time = time.time() - start_time
    
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
    
    if device == 'cuda':
        model = model.cuda()
        print("âœ“ ëª¨ë¸ì„ GPUë¡œ ì´ë™")
        
        # CUDA ìºì‹œ ì›Œë°ì—… (ë§¤ìš° ì¤‘ìš”!)
        print("âœ“ CUDA ìºì‹œ ì›Œë°ì—… ì¤‘...")
        try:
            # Decoderì˜ coord cacheë¥¼ CUDAì— ìƒì„±
            if hasattr(model, 'transformer') and hasattr(model.transformer, 'decoder'):
                decoder = model.transformer.decoder
                target_device = torch.device('cuda')
                
                # compilable_cord_cacheë¥¼ CUDAë¡œ ê°•ì œ ì´ë™
                if hasattr(decoder, 'compilable_cord_cache') and decoder.compilable_cord_cache is not None:
                    coords_h, coords_w = decoder.compilable_cord_cache
                    decoder.compilable_cord_cache = (
                        coords_h.to(target_device),
                        coords_w.to(target_device)
                    )
                    print(f"  - compilable_cord_cache â†’ {target_device}")
                
                # coord_cache ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  ì—”íŠ¸ë¦¬ë¥¼ CUDAë¡œ ì´ë™
                if hasattr(decoder, 'coord_cache'):
                    for feat_size, (coords_h, coords_w) in list(decoder.coord_cache.items()):
                        decoder.coord_cache[feat_size] = (
                            coords_h.to(target_device),
                            coords_w.to(target_device)
                        )
                    if decoder.coord_cache:
                        print(f"  - coord_cache ({len(decoder.coord_cache)} entries) â†’ {target_device}")
                
                # Monkey patch _get_rpb_matrix to ensure device consistency
                original_get_rpb_matrix = decoder._get_rpb_matrix
                
                def patched_get_rpb_matrix(reference_boxes, feat_size):
                    """Patched version that ensures coords are on same device as reference_boxes"""
                    H, W = feat_size
                    boxes_xyxy = box_cxcywh_to_xyxy(reference_boxes).transpose(0, 1)
                    bs, num_queries, _ = boxes_xyxy.shape
                    
                    # Get device from reference_boxes
                    target_dev = reference_boxes.device
                    
                    # Check cache first
                    if decoder.compilable_cord_cache is None:
                        coords_h, coords_w = decoder._get_coords(H, W, target_dev)
                        decoder.compilable_cord_cache = (coords_h, coords_w)
                        decoder.compilable_stored_size = (H, W)
                    
                    if torch.compiler.is_dynamo_compiling() or decoder.compilable_stored_size == (H, W):
                        coords_h, coords_w = decoder.compilable_cord_cache
                        # Ensure on correct device
                        if coords_h.device != target_dev:
                            coords_h = coords_h.to(target_dev)
                            coords_w = coords_w.to(target_dev)
                            decoder.compilable_cord_cache = (coords_h, coords_w)
                    else:
                        if feat_size not in decoder.coord_cache:
                            decoder.coord_cache[feat_size] = decoder._get_coords(H, W, target_dev)
                        coords_h, coords_w = decoder.coord_cache[feat_size]
                        # Ensure on correct device
                        if coords_h.device != target_dev:
                            coords_h = coords_h.to(target_dev)
                            coords_w = coords_w.to(target_dev)
                            decoder.coord_cache[feat_size] = (coords_h, coords_w)
                    
                    # Continue with original logic
                    deltas_y = coords_h.view(1, -1, 1) - boxes_xyxy.reshape(-1, 1, 4)[:, :, 1:4:2]
                    deltas_y = deltas_y.view(bs, num_queries, -1, 2)
                    deltas_x = coords_w.view(1, -1, 1) - boxes_xyxy.reshape(-1, 1, 4)[:, :, 0:3:2]
                    deltas_x = deltas_x.view(bs, num_queries, -1, 2)
                    
                    if decoder.boxRPB in ["log", "both"]:
                        deltas_x_log = deltas_x * 8
                        deltas_x_log = (
                            torch.sign(deltas_x_log)
                            * torch.log2(torch.abs(deltas_x_log) + 1.0)
                            / np.log2(8)
                        )
                        deltas_y_log = deltas_y * 8
                        deltas_y_log = (
                            torch.sign(deltas_y_log)
                            * torch.log2(torch.abs(deltas_y_log) + 1.0)
                            / np.log2(8)
                        )
                        if decoder.boxRPB == "log":
                            deltas_x = deltas_x_log
                            deltas_y = deltas_y_log
                        else:
                            deltas_x = torch.cat([deltas_x, deltas_x_log], dim=-1)
                            deltas_y = torch.cat([deltas_y, deltas_y_log], dim=-1)
                    
                    deltas_x = decoder.boxRPB_embed_x(deltas_x)
                    deltas_y = decoder.boxRPB_embed_y(deltas_y)
                    
                    B = deltas_y.unsqueeze(3) + deltas_x.unsqueeze(2)
                    B = B.flatten(2, 3)
                    B = B.permute(0, 3, 1, 2)
                    B = B.contiguous()
                    return B
                
                # Import box_cxcywh_to_xyxy for the patched function
                from sam3.model.box_ops import box_cxcywh_to_xyxy
                import numpy as np
                
                decoder._get_rpb_matrix = patched_get_rpb_matrix
                print(f"  - _get_rpb_matrix patched for device consistency")
                
        except Exception as e:
            print(f"  âš  ìºì‹œ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            import traceback
            traceback.print_exc()
    else:
        model = model.cpu()
        print("âœ“ ëª¨ë¸ì„ CPUì— ìœ ì§€")
    
    print()
    return model


def create_transforms():
    """ì „ì²˜ë¦¬ Transform ìƒì„±"""
    return ComposeAPI(
        transforms=[
            RandomResizeAPI(sizes=1008, max_size=1008, square=True, consistent_transform=False),
            ToTensorAPI(),
            NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ]
    )


def create_postprocessor(detection_threshold=0.3, device='cuda'):
    """í›„ì²˜ë¦¬ PostProcessor ìƒì„±"""
    # CPU ëª¨ë“œì—ì„œëŠ” GPU ê´€ë ¨ ì„¤ì • ë¹„í™œì„±í™”
    if device == 'cpu':
        return PostProcessImage(
            max_dets_per_img=-1,
            iou_type="segm",
            use_original_sizes_box=True,
            use_original_sizes_mask=True,
            convert_mask_to_rle=False,
            detection_threshold=detection_threshold,
            to_cpu=True,
            always_interpolate_masks_on_gpu=False
        )
    else:
        return PostProcessImage(
            max_dets_per_img=-1,
            iou_type="segm",
            use_original_sizes_box=True,
            use_original_sizes_mask=True,
            convert_mask_to_rle=False,
            detection_threshold=detection_threshold,
            to_cpu=False,
        )


def create_datapoint_with_prompts(pil_image, text_prompts):
    """
    ì´ë¯¸ì§€ì™€ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œ Datapoint ìƒì„±
    
    Args:
        pil_image: PIL Image
        text_prompts: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        datapoint: Datapoint ê°ì²´
        prompt_ids: ê° í”„ë¡¬í”„íŠ¸ì˜ ID ë¦¬ìŠ¤íŠ¸
    """
    global GLOBAL_COUNTER
    
    datapoint = Datapoint(find_queries=[], images=[])
    
    # ì´ë¯¸ì§€ ì„¤ì •
    w, h = pil_image.size
    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h, w])]
    
    # ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    prompt_ids = []
    for text_query in text_prompts:
        datapoint.find_queries.append(
            FindQueryLoaded(
                query_text=text_query,
                image_id=0,
                object_ids_output=[],
                is_exhaustive=True,
                query_processing_order=0,
                inference_metadata=InferenceMetadata(
                    coco_image_id=GLOBAL_COUNTER,
                    original_image_id=GLOBAL_COUNTER,
                    original_category_id=1,
                    original_size=[h, w],  # height, width ìˆœì„œ!
                    object_id=0,
                    frame_index=0,
                )
            )
        )
        prompt_ids.append(GLOBAL_COUNTER)
        GLOBAL_COUNTER += 1
    
    return datapoint, prompt_ids


def parse_image_source(image_source):
    """ì´ë¯¸ì§€ ì†ŒìŠ¤ íŒŒì‹±"""
    image_paths = []
    
    if os.path.isdir(image_source):
        print(f"ğŸ“ í´ë”ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰: {image_source}")
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        
        image_paths_set = set()
        for ext in extensions:
            image_paths_set.update(Path(image_source).glob(f"*{ext}"))
            image_paths_set.update(Path(image_source).glob(f"*{ext.upper()}"))
        
        image_paths = [str(p) for p in sorted(image_paths_set)]
        print(f"  âœ“ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    elif os.path.isfile(image_source):
        print(f"ğŸ“„ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ: {image_source}")
        
        with open(image_source, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and os.path.exists(line):
                    image_paths.append(line)
        
        print(f"  âœ“ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ")
    
    else:
        raise ValueError(f"ì˜ëª»ëœ ì†ŒìŠ¤: {image_source}")
    
    if len(image_paths) == 0:
        raise ValueError("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return sorted(image_paths)


def parse_video_source(video_source):
    """ë™ì˜ìƒ ì†ŒìŠ¤ íŒŒì‹± (íŒŒì¼ ë˜ëŠ” í´ë”)"""
    video_paths = []
    
    if os.path.isdir(video_source):
        print(f"ğŸ“ í´ë”ì—ì„œ ë™ì˜ìƒ ê²€ìƒ‰: {video_source}")
        extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
        
        video_paths_set = set()
        for ext in extensions:
            video_paths_set.update(Path(video_source).glob(f"*{ext}"))
            video_paths_set.update(Path(video_source).glob(f"*{ext.upper()}"))
        
        video_paths = [str(p) for p in sorted(video_paths_set)]
        print(f"  âœ“ {len(video_paths)}ê°œ ë™ì˜ìƒ ë°œê²¬")
    
    elif os.path.isfile(video_source):
        print(f"ğŸ“„ ë™ì˜ìƒ íŒŒì¼: {video_source}")
        video_paths = [video_source]
    
    else:
        raise ValueError(f"ì˜ëª»ëœ ë™ì˜ìƒ ì†ŒìŠ¤: {video_source}")
    
    if len(video_paths) == 0:
        raise ValueError("ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return sorted(video_paths)


def parse_class_mapping(class_str):
    """í´ë˜ìŠ¤ ë§¤í•‘ íŒŒì‹±"""
    if isinstance(class_str, dict):
        return class_str
    
    mapping = {}
    pairs = class_str.split(',')
    
    for pair in pairs:
        pair = pair.strip()
        if ':' in pair:
            name, idx = pair.split(':')
            mapping[name.strip()] = int(idx.strip())
    
    return mapping


def bbox_to_yolo_format(box, img_width, img_height):
    """ë°•ìŠ¤ë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    x1, y1, x2, y2 = box
    
    x_center = (x1 + x2) / 2.0 / img_width
    y_center = (y1 + y2) / 2.0 / img_height
    width = (x2 - x1) / img_width
    height = (y2 - y1) / img_height
    
    return x_center, y_center, width, height


def save_yolo_annotation(image_path, results_by_prompt, class_mapping, output_dir, img_width, img_height):
    """YOLO í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ì €ì¥"""
    image_name = Path(image_path).stem
    txt_path = os.path.join(output_dir, f"{image_name}.txt")
    
    lines = []
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        class_id = class_mapping.get(prompt_name, -1)
        if class_id < 0:
            continue
        
        boxes = result['boxes']
        scores = result['scores']
        
        for idx, (box, score) in enumerate(zip(boxes, scores)):
            # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            x_center, y_center, width, height = bbox_to_yolo_format(
                box, img_width, img_height
            )
            
            # 5ê°œ ê°’ë§Œ ì €ì¥
            line = f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"
            lines.append(line)
            total_objects += 1
    
    # íŒŒì¼ ì €ì¥
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    
    return total_objects


def imread_unicode(image_path):
    """í•œê¸€ ê²½ë¡œ ì§€ì› ì´ë¯¸ì§€ ì½ê¸°"""
    import cv2
    import numpy as np
    
    try:
        stream = open(image_path, "rb")
        bytes_data = bytearray(stream.read())
        numpy_array = np.asarray(bytes_data, dtype=np.uint8)
        image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
        stream.close()
        return image
    except:
        return None


def imwrite_unicode(image_path, image):
    """í•œê¸€ ê²½ë¡œ ì§€ì› ì´ë¯¸ì§€ ì €ì¥"""
    import cv2
    
    try:
        ext = os.path.splitext(image_path)[1]
        result, encoded_img = cv2.imencode(ext, image)
        if result:
            with open(image_path, mode='w+b') as f:
                encoded_img.tofile(f)
            return True
        return False
    except:
        return False


def save_visualization_result(image_path, results_by_prompt, class_mapping, output_dir):
    """ì‹œê°í™” ê²°ê³¼ ì €ì¥"""
    import cv2
    
    image = imread_unicode(image_path)
    if image is None:
        return None
    
    colors = [
        (0, 255, 0),    # ë…¹ìƒ‰
        (255, 0, 0),    # íŒŒë€ìƒ‰
        (0, 0, 255),    # ë¹¨ê°„ìƒ‰
        (0, 255, 255),  # ë…¸ë€ìƒ‰
        (255, 0, 255),  # ë§ˆì  íƒ€
        (255, 255, 0),  # ì‹œì•ˆ
    ]
    
    color_idx = 0
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        color = colors[color_idx % len(colors)]
        color_idx += 1
        
        boxes = result['boxes']
        scores = result['scores']
        
        for box, score in zip(boxes, scores):
            x1, y1, x2, y2 = [int(v) for v in box]
            
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            
            label = f"{prompt_name}: {score:.2f}"
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            cv2.rectangle(
                image,
                (x1, y1 - text_height - 10),
                (x1 + text_width, y1),
                (0, 0, 0),
                -1
            )
            
            cv2.putText(
                image,
                label,
                (x1, y1 - 5),
                font,
                font_scale,
                (255, 255, 255),
                thickness
            )
            
            total_objects += 1
    
    info_text = f"{Path(image_path).name} | Objects: {total_objects}"
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    os.makedirs(output_dir, exist_ok=True)
    image_name = Path(image_path).stem
    save_path = os.path.join(output_dir, f"{image_name}_result.jpg")
    imwrite_unicode(save_path, image)
    
    return save_path


def show_realtime_result(image_path, results_by_prompt, class_mapping, window_name="SAM3 Detection"):
    """ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ"""
    import cv2
    
    image = imread_unicode(image_path)
    if image is None:
        return
    
    img_h, img_w = image.shape[:2]
    
    colors = [
        (0, 255, 0),
        (255, 0, 0),
        (0, 0, 255),
        (0, 255, 255),
        (255, 0, 255),
        (255, 255, 0),
    ]
    
    color_idx = 0
    total_objects = 0
    
    for prompt_name, result in results_by_prompt.items():
        if result is None or len(result['boxes']) == 0:
            continue
        
        color = colors[color_idx % len(colors)]
        color_idx += 1
        
        boxes = result['boxes']
        scores = result['scores']
        
        for idx, (box, score) in enumerate(zip(boxes, scores)):
            x1, y1, x2, y2 = [int(v) for v in box]
            
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            
            label = f"{prompt_name}: {score:.2f}"
            
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                label, font, font_scale, thickness
            )
            
            cv2.rectangle(
                image,
                (x1, y1 - text_height - 10),
                (x1 + text_width, y1),
                (0, 0, 0),
                -1
            )
            
            cv2.putText(
                image,
                label,
                (x1, y1 - 5),
                font,
                font_scale,
                (255, 255, 255),
                thickness
            )
            
            total_objects += 1
    
    info_text = f"{Path(image_path).name} | Objects: {total_objects}"
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    cv2.resizeWindow(window_name, 1280, 720)
    cv2.imshow(window_name, image)
    cv2.waitKey(200)


def extract_frames_from_videos(video_source, jpeg_output_dir, fps_extraction=1, verbose=True):
    """
    ë™ì˜ìƒì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ë° JPEGImages ì €ì¥
    
    Args:
        video_source: ë™ì˜ìƒ íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ
        jpeg_output_dir: JPEGImages ì €ì¥ ê²½ë¡œ
        fps_extraction: ì¶”ì¶œ FPS
                       - 1, 5, 30 ë“±: 1ì´ˆë‹¹ Ní”„ë ˆì„ ì¶”ì¶œ
                       - 0 ë˜ëŠ” -1: ì›ë³¸ FPS ì „ì²´ í”„ë ˆì„
        verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        extracted_count: ì¶”ì¶œëœ ì´ í”„ë ˆì„ ìˆ˜
    """
    import cv2
    
    print("\n" + "=" * 60)
    print("ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘")
    print("=" * 60)
    
    # ë™ì˜ìƒ íŒŒì¼ íŒŒì‹±
    video_paths = parse_video_source(video_source)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(jpeg_output_dir, exist_ok=True)
    print(f"ğŸ“ JPEGImages ì €ì¥ ê²½ë¡œ: {jpeg_output_dir}\n")
    
    total_extracted = 0
    global_frame_index = 1  # ì „ì²´ í”„ë ˆì„ ì¸ë±ìŠ¤ (ëª¨ë“  ë™ì˜ìƒ í†µí•©)
    
    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False
    
    for video_idx, video_path in enumerate(video_paths):
        video_name = Path(video_path).stem
        
        if verbose:
            print(f"\n[{video_idx+1}/{len(video_paths)}] {video_name}")
        
        # OpenCVë¡œ ë™ì˜ìƒ ì—´ê¸°
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"  âœ— ë™ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            continue
        
        # ë™ì˜ìƒ ì •ë³´
        original_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / original_fps if original_fps > 0 else 0
        
        if verbose:
            print(f"  ì›ë³¸ FPS: {original_fps:.2f}")
            print(f"  ì´ í”„ë ˆì„: {total_frames}")
            print(f"  ê¸¸ì´: {duration:.2f}ì´ˆ")
        
        # FPS ì¶”ì¶œ ë¡œì§ ê²°ì •
        if fps_extraction <= 0:
            # ì›ë³¸ FPS ì „ì²´ ì¶”ì¶œ
            frame_interval = 1
            extract_fps = original_fps
            if verbose:
                print(f"  ì¶”ì¶œ ëª¨ë“œ: ì›ë³¸ FPS ì „ì²´ ({original_fps:.2f}fps)")
        else:
            # ì§€ì •ëœ FPSë¡œ ì¶”ì¶œ
            frame_interval = int(original_fps / fps_extraction)
            if frame_interval < 1:
                frame_interval = 1
            extract_fps = fps_extraction
            if verbose:
                print(f"  ì¶”ì¶œ ëª¨ë“œ: {fps_extraction}fps (ë§¤ {frame_interval}í”„ë ˆì„)")
        
        # ì˜ˆìƒ ì¶”ì¶œ í”„ë ˆì„ ìˆ˜
        estimated_frames = total_frames // frame_interval
        if verbose:
            print(f"  ì˜ˆìƒ ì¶”ì¶œ: {estimated_frames}í”„ë ˆì„\n")
        
        # í”„ë ˆì„ ì¶”ì¶œ
        frame_count = 0
        extracted_count = 0
        
        iterator = tqdm(total=total_frames, desc=f"  {video_name}") if use_tqdm else range(total_frames)
        
        while True:
            ret, frame = cap.read()
            
            if not ret:
                break
            
            # í”„ë ˆì„ ê°„ê²©ì— ë§ì¶° ì¶”ì¶œ
            if frame_count % frame_interval == 0:
                # íŒŒì¼ëª…: video_name_frame_000001.jpg
                frame_filename = f"{video_name}_frame_{global_frame_index:06d}.jpg"
                frame_path = os.path.join(jpeg_output_dir, frame_filename)
                
                # í•œê¸€ ê²½ë¡œ ì§€ì› ì €ì¥
                success = imwrite_unicode(frame_path, frame)
                
                if success:
                    extracted_count += 1
                    global_frame_index += 1
            
            frame_count += 1
            
            if use_tqdm:
                iterator.update(1)
        
        if use_tqdm:
            iterator.close()
        
        cap.release()
        
        if verbose:
            print(f"  âœ“ ì¶”ì¶œ ì™„ë£Œ: {extracted_count}í”„ë ˆì„")
        
        total_extracted += extracted_count
    
    print("\n" + "=" * 60)
    print("í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ!")
    print("=" * 60)
    print(f"âœ“ ì²˜ë¦¬ ë™ì˜ìƒ: {len(video_paths)}ê°œ")
    print(f"âœ“ ì¶”ì¶œ í”„ë ˆì„: {total_extracted}ê°œ")
    print(f"âœ“ ì €ì¥ ê²½ë¡œ: {jpeg_output_dir}")
    print("=" * 60 + "\n")
    
    return total_extracted


def process_single_image_batch(
    image_path, model, transform, postprocessor, prompts, 
    class_mapping, output_dir, device='cuda',
    show_realtime=False, save_visualizations=False, 
    visualization_dir=None, window_name="SAM3 Detection",
    prompt_chunk_size=4
):
    """ë‹¨ì¼ ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        total_start = time.time()
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        load_start = time.time()
        pil_image = Image.open(image_path)

        if pil_image.mode != "RGB":
            pil_image = pil_image.convert("RGB")

        # EXIF ì™„ì „ ì œê±° (numpy ê²½ìœ )
        pil_image = Image.fromarray(np.array(pil_image))

        img_width, img_height = pil_image.size
        load_time = time.time() - load_start
        
        # ì „ì²´ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        results_by_prompt = {}
        
        # íƒ€ì´ë° ëˆ„ì 
        total_prep_time = 0
        total_inference_time = 0
        total_post_time = 0
        
        # í”„ë¡¬í”„íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
        num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
        
        for chunk_idx in range(num_chunks):
            # í˜„ì¬ ì²­í¬ì˜ í”„ë¡¬í”„íŠ¸
            start_idx = chunk_idx * prompt_chunk_size
            end_idx = min(start_idx + prompt_chunk_size, len(prompts))
            chunk_prompts = prompts[start_idx:end_idx]
            
            # Datapoint ìƒì„± (ì²­í¬ ë‹¨ìœ„ í”„ë¡¬í”„íŠ¸)
            prep_start = time.time()
            datapoint, prompt_ids = create_datapoint_with_prompts(pil_image, chunk_prompts)
            
            # Transform ì ìš©
            datapoint = transform(datapoint)
            
            # ë°°ì¹˜ë¡œ collate
            batch = collate([datapoint], dict_key="dummy")["dummy"]
            
            # Deviceë¡œ ì™„ì „íˆ ì´ë™
            target_device = torch.device(device)
            
            # Method 1: ê¸°ë³¸ ì´ë™
            batch = copy_data_to_device(batch, target_device, non_blocking=True)
            
            # Method 2: ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í…ì„œ ì´ë™ (ì•ˆì „ì¥ì¹˜)
            batch = recursive_to_device(batch, target_device)
            
            prep_time = time.time() - prep_start
            total_prep_time += prep_time
            
            # GPU ë™ê¸°í™” (ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´)
            if device == 'cuda':
                torch.cuda.synchronize()
            
            # ì²­í¬ ì¶”ë¡ 
            inference_start = time.time()
            output = model(batch)
            
            # GPU ë™ê¸°í™” (ì¶”ë¡  ì™„ë£Œ ëŒ€ê¸°)
            if device == 'cuda':
                torch.cuda.synchronize()
            inference_time = time.time() - inference_start
            total_inference_time += inference_time
            
            # í›„ì²˜ë¦¬
            post_start = time.time()
            processed_results = postprocessor.process_results(output, batch.find_metadatas)
            post_time = time.time() - post_start
            total_post_time += post_time
            
            # ê²°ê³¼ ì •ë¦¬ (prompt_idë³„ë¡œ ë¶„ë¥˜)
            if not isinstance(processed_results, list):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                if isinstance(processed_results, dict):
                    # ê° query_idë³„ë¡œ ê²°ê³¼ ì¶”ì¶œ
                    for prompt_name, prompt_id in zip(chunk_prompts, prompt_ids):
                        if prompt_id in processed_results:
                            result = processed_results[prompt_id]
                            # bfloat16 â†’ float32 â†’ numpy
                            boxes = result['boxes'].float().cpu().numpy() if hasattr(result['boxes'], 'cpu') else result['boxes']
                            scores = result['scores'].float().cpu().numpy() if hasattr(result['scores'], 'cpu') else result['scores']
                            
                            results_by_prompt[prompt_name] = {
                                'boxes': boxes,
                                'scores': scores
                            }
                        else:
                            # ê²€ì¶œ ê²°ê³¼ ì—†ìŒ
                            if prompt_name not in results_by_prompt:
                                results_by_prompt[prompt_name] = {
                                    'boxes': np.array([]),
                                    'scores': np.array([])
                                }
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ - ë¹ˆ ê²°ê³¼ ë°˜í™˜
                    for prompt_name in chunk_prompts:
                        if prompt_name not in results_by_prompt:
                            results_by_prompt[prompt_name] = {
                                'boxes': np.array([]),
                                'scores': np.array([])
                            }
            else:
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
                for result in processed_results:
                    # resultê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                    if isinstance(result, dict) and 'query_id' in result:
                        query_id = result['query_id']
                        
                        # query_idë¡œ í”„ë¡¬í”„íŠ¸ ì°¾ê¸°
                        for prompt_name, prompt_id in zip(chunk_prompts, prompt_ids):
                            if query_id == prompt_id:
                                # bfloat16 â†’ float32 â†’ numpy
                                boxes = result['boxes'].float().cpu().numpy() if hasattr(result['boxes'], 'cpu') else result['boxes']
                                scores = result['scores'].float().cpu().numpy() if hasattr(result['scores'], 'cpu') else result['scores']
                                results_by_prompt[prompt_name] = {
                                    'boxes': boxes,
                                    'scores': scores
                                }
                                break
                
                # í˜„ì¬ ì²­í¬ì—ì„œ ê²°ê³¼ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ëŠ” ë¹ˆ ë°°ì—´
                for prompt_name in chunk_prompts:
                    if prompt_name not in results_by_prompt:
                        results_by_prompt[prompt_name] = {
                            'boxes': np.array([]),
                            'scores': np.array([])
                        }
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if device == 'cuda':
                del batch, output, processed_results
                torch.cuda.empty_cache()
        
        # YOLO ì–´ë…¸í…Œì´ì…˜ ì €ì¥
        save_start = time.time()
        num_objects = save_yolo_annotation(
            image_path, results_by_prompt, class_mapping, 
            output_dir, img_width, img_height
        )
        save_time = time.time() - save_start
        
        # ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ
        if show_realtime:
            show_realtime_result(image_path, results_by_prompt, class_mapping, window_name)
        
        # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥
        visualization_path = None
        if save_visualizations and visualization_dir:
            visualization_path = save_visualization_result(
                image_path, results_by_prompt, class_mapping, visualization_dir
            )
        
        # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
        total_time = time.time() - total_start
        
        return {
            'success': True,
            'num_objects': num_objects,
            'image_size': (img_width, img_height),
            'visualization_path': visualization_path,
            'timing': {
                'total': total_time,
                'load': load_time,
                'preprocess': total_prep_time,
                'inference': total_inference_time,
                'postprocess': total_post_time,
                'save': save_time
            },
            'num_chunks': num_chunks
        }
        
    except Exception as e:
        import traceback
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {image_path}")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'num_objects': 0,
            'visualization_path': None
        }


def create_yolo_dataset(
    image_source,
    output_dir,
    class_mapping,
    bpe_path,
    prompts=None,
    model=None,
    transform=None,
    postprocessor=None,
    detection_threshold=0.3,
    device='cuda',
    prompt_chunk_size=4,
    verbose=True,
    show_realtime=False,
    save_visualizations=False,
    visualization_dir=None
):
    """
    YOLO í˜•ì‹ ë°ì´í„°ì…‹ ìƒì„± (ë°°ì¹˜ ì¶”ë¡  ìµœì í™” + ì²­í¬ ì²˜ë¦¬)
    
    Args:
        prompt_chunk_size: í•œë²ˆì— ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸ ê°œìˆ˜ (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
                          - RTX 4090 24GB: 6-8 ì¶”ì²œ
                          - RTX 3090 24GB: 4-6 ì¶”ì²œ
                          - RTX 3080 10GB: 2-4 ì¶”ì²œ
    """
    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 12 + "YOLO Dataset Creation Tool" + " " * 12 + "â•‘")
    print("â•‘" + " " * 15 + "(Batch Inference Mode)" + " " * 15 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()
    
    start_time = time.time()
    
    # í´ë˜ìŠ¤ ë§¤í•‘ íŒŒì‹±
    print("=" * 60)
    print("ì„¤ì • í™•ì¸")
    print("=" * 60)
    
    class_mapping = parse_class_mapping(class_mapping)
    print(f"í´ë˜ìŠ¤ ë§¤í•‘: {class_mapping}")
    
    if prompts is None:
        prompts = list(class_mapping.keys())
    print(f"í”„ë¡¬í”„íŠ¸: {prompts}")
    print(f"í”„ë¡¬í”„íŠ¸ ì´ ê°œìˆ˜: {len(prompts)}ê°œ")
    print(f"í”„ë¡¬í”„íŠ¸ ì²­í¬ í¬ê¸°: {prompt_chunk_size}ê°œ (GPU ë©”ëª¨ë¦¬ ì ˆì•½)")
    print(f"ì²­í¬ ìˆ˜: {(len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size}ê°œ")
    print(f"ê²€ì¶œ ì„ê³„ê°’: {detection_threshold}")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì‹¤ì‹œê°„ í‘œì‹œ: {show_realtime}")
    print(f"ì‹œê°í™” ì €ì¥: {save_visualizations}")
    if save_visualizations and visualization_dir:
        print(f"ì‹œê°í™” ë””ë ‰í† ë¦¬: {visualization_dir}")
    print()
    
    # ì´ë¯¸ì§€ ì†ŒìŠ¤ íŒŒì‹±
    print("=" * 60)
    print("ì´ë¯¸ì§€ ë¡œë“œ")
    print("=" * 60)
    image_paths = parse_image_source(image_source)
    print()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ ë¼ë²¨ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    if save_visualizations and visualization_dir:
        os.makedirs(visualization_dir, exist_ok=True)
        print(f"ğŸ“ ì‹œê°í™” ì¶œë ¥ ë””ë ‰í† ë¦¬: {visualization_dir}")
    print()
    
    # ëª¨ë¸ ë¡œë“œ
    if model is None:
        model = load_model(bpe_path, device)
    
    if transform is None:
        transform = create_transforms()
        print("âœ“ Transform ìƒì„± ì™„ë£Œ")
    
    if postprocessor is None:
        postprocessor = create_postprocessor(detection_threshold, device)
        print("âœ“ PostProcessor ìƒì„± ì™„ë£Œ")
    
    print()
    
    # ì‹¤ì‹œê°„ í‘œì‹œìš© ìœˆë„ìš° ìƒì„±
    window_name = "SAM3 Real-time Detection"
    if show_realtime:
        import cv2
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        print("ğŸ–¥ï¸  ì‹¤ì‹œê°„ í‘œì‹œ ìœˆë„ìš° ìƒì„±\n")
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    print("=" * 60)
    print("ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ ì¶”ë¡  ëª¨ë“œ + ì²­í¬ ì²˜ë¦¬)")
    print("=" * 60)
    num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
    print(f"âœ“ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œë¥¼ {num_chunks}ê°œ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬")
    print(f"âœ“ ì²­í¬ë‹¹ {prompt_chunk_size}ê°œ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ì²˜ë¦¬")
    print(f"âœ“ ì´ë¯¸ì§€ë‹¹ ì´ {num_chunks}ë²ˆ forward\n")
    
    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False
    
    total_objects = 0
    success_count = 0
    fail_count = 0
    
    # íƒ€ì´ë° í†µê³„
    timing_stats = {
        'total': [],
        'load': [],
        'preprocess': [],
        'inference': [],
        'postprocess': [],
        'save': []
    }
    
    # ì²« ì¶”ë¡  ì‹œê°„ (ì»´íŒŒì¼ í¬í•¨)
    first_inference_time = None
    
    iterator = tqdm(image_paths, desc="Processing") if use_tqdm else image_paths
    
    for idx, image_path in enumerate(iterator):
        if not use_tqdm and verbose:
            print(f"[{idx+1}/{len(image_paths)}] {Path(image_path).name}", end=" ... ")
        
        result = process_single_image_batch(
            image_path, model, transform, postprocessor, prompts,
            class_mapping, output_dir, device,
            show_realtime=show_realtime,
            save_visualizations=save_visualizations,
            visualization_dir=visualization_dir,
            window_name=window_name,
            prompt_chunk_size=prompt_chunk_size
        )
        
        if result['success']:
            success_count += 1
            total_objects += result['num_objects']
            
            # íƒ€ì´ë° í†µê³„ ìˆ˜ì§‘
            timing = result.get('timing', {})
            for key in timing_stats.keys():
                if key in timing:
                    timing_stats[key].append(timing[key])
            
            # ì²« ì¶”ë¡  ì‹œê°„ ê¸°ë¡ (ì»´íŒŒì¼ í¬í•¨)
            if first_inference_time is None and 'inference' in timing:
                first_inference_time = timing['inference']
            
            if not use_tqdm and verbose:
                print(f"âœ“ ({result['num_objects']} objects, {timing.get('inference', 0):.3f}s)")
        else:
            fail_count += 1
            if not use_tqdm and verbose:
                print(f"âœ— {result['error']}")
    
    # ìœˆë„ìš° ì •ë¦¬
    if show_realtime:
        import cv2
        cv2.destroyAllWindows()
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ì™„ë£Œ!")
    print("=" * 60)
    print(f"âœ“ ì„±ê³µ: {success_count}/{len(image_paths)} ì´ë¯¸ì§€")
    print(f"âœ“ ì´ ê°ì²´ ìˆ˜: {total_objects}")
    print(f"âœ“ í‰ê·  ê°ì²´/ì´ë¯¸ì§€: {total_objects/max(success_count,1):.1f}")
    print(f"âœ“ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}ê°œ")
    num_chunks = (len(prompts) + prompt_chunk_size - 1) // prompt_chunk_size
    print(f"âœ“ ì²­í¬ ì²˜ë¦¬ ëª¨ë“œ: ì´ë¯¸ì§€ë‹¹ {num_chunks}ë²ˆ forward ({prompt_chunk_size}ê°œì”©)")
    
    if fail_count > 0:
        print(f"âœ— ì‹¤íŒ¨: {fail_count} ì´ë¯¸ì§€")
    
    print(f"\nâ±  ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"â±  ì´ë¯¸ì§€ë‹¹ í‰ê· : {elapsed_time/len(image_paths):.2f}ì´ˆ")
    
    # íƒ€ì´ë° í†µê³„ ì¶œë ¥
    if timing_stats['inference']:
        print("\n" + "=" * 60)
        print("ğŸ“Š ìƒì„¸ íƒ€ì´ë° í†µê³„ (í‰ê· )")
        print("=" * 60)
        
        # ì²« ì¶”ë¡  ì‹œê°„ (ì»´íŒŒì¼ í¬í•¨)
        if first_inference_time is not None:
            print(f"ì²« ì¶”ë¡  ì‹œê°„ (ì»´íŒŒì¼ í¬í•¨): {first_inference_time:.3f}ì´ˆ")
        
        # ë‘ ë²ˆì§¸ ì´í›„ ì¶”ë¡  í‰ê·  (ì»´íŒŒì¼ ì œì™¸)
        if len(timing_stats['inference']) > 1:
            avg_inference_without_compile = np.mean(timing_stats['inference'][1:])
            print(f"ì´í›„ ì¶”ë¡  í‰ê·  (ì»´íŒŒì¼ ì œì™¸): {avg_inference_without_compile:.3f}ì´ˆ")
        
        print(f"\nê° ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„:")
        print(f"  - ì´ë¯¸ì§€ ë¡œë“œ:     {np.mean(timing_stats['load']):.3f}ì´ˆ")
        print(f"  - ì „ì²˜ë¦¬:          {np.mean(timing_stats['preprocess']):.3f}ì´ˆ")
        print(f"  - ì¶”ë¡  (forward):  {np.mean(timing_stats['inference']):.3f}ì´ˆ")
        print(f"  - í›„ì²˜ë¦¬:          {np.mean(timing_stats['postprocess']):.3f}ì´ˆ")
        print(f"  - ì €ì¥:            {np.mean(timing_stats['save']):.3f}ì´ˆ")
        print(f"  - ì „ì²´:            {np.mean(timing_stats['total']):.3f}ì´ˆ")
        
        # ì¶”ë¡  ì†ë„ ë¶„ì„
        print(f"\nì¶”ë¡  ì„±ëŠ¥ ë¶„ì„:")
        inference_times = timing_stats['inference']
        print(f"  - ìµœì†Œ: {np.min(inference_times):.3f}ì´ˆ")
        print(f"  - ìµœëŒ€: {np.max(inference_times):.3f}ì´ˆ")
        print(f"  - í‰ê· : {np.mean(inference_times):.3f}ì´ˆ")
        print(f"  - ì¤‘ì•™ê°’: {np.median(inference_times):.3f}ì´ˆ")
        
        # GPU vs CPU í‘œì‹œ
        print(f"\në””ë°”ì´ìŠ¤: {device.upper()}")
        if device == 'cuda':
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
        
        # í”„ë¡¬í”„íŠ¸ë‹¹ ì¶”ë¡  ì‹œê°„
        avg_inference = np.mean(inference_times)
        time_per_prompt = avg_inference / len(prompts)
        print(f"\níš¨ìœ¨ì„±:")
        print(f"  - í”„ë¡¬í”„íŠ¸ë‹¹ í‰ê·  ì‹œê°„: {time_per_prompt:.3f}ì´ˆ")
        print(f"  - ì´ˆë‹¹ ì²˜ë¦¬ í”„ë¡¬í”„íŠ¸: {1/time_per_prompt:.1f}ê°œ")
    
    print(f"\nğŸ“ ë¼ë²¨ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    if save_visualizations and visualization_dir:
        print(f"ğŸ“ ì‹œê°í™” ì €ì¥ ìœ„ì¹˜: {visualization_dir}")
    print("=" * 60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 5 + "SAM 3 YOLO Dataset Creator (Video Support)" + " " * 5 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()
    
    # HuggingFace í† í°
    #HF_TOKEN = ""
    
    # SAM3 ë£¨íŠ¸ ê²½ë¡œ
    import sam3
    sam3_root = os.path.join(os.path.dirname(sam3.__file__), "..")
    bpe_path = os.path.join(sam3_root, "assets", "bpe_simple_vocab_16e6.txt.gz")
    
    # ë°ì´í„°ì…‹ ìƒì„± ì„¤ì •
    DATASET_CONFIG = {
        # ë™ì˜ìƒ ê´€ë ¨ ì„¤ì • (NEW!)
        "video_source": None,  # ë™ì˜ìƒ íŒŒì¼/í´ë” ê²½ë¡œ (Noneì´ë©´ ìŠ¤í‚µ)
        "jpeg_output_dir": "X:/ë°•ì°½í˜„/pipe_lower_part/data/JPEGImages",  # í”„ë ˆì„ ì €ì¥ ê²½ë¡œ
        "fps_extraction": 1,  # ì¶”ì¶œ FPS (1=1fps, 5=5fps, 0/-1=ì›ë³¸ ì „ì²´)
        
        # ê¸°ì¡´ ì„¤ì •
        "image_source": "X:/ë°•ì°½í˜„/pipe_lower_part/data/JPEGImages",
        "label_dir": "X:/ë°•ì°½í˜„/pipe_lower_part/data/labels",
        "class_mapping": {
            "industrial hose": 0,
            "Chemical Hose": 1,
            "flexible duct": 2
        },
        "detection_threshold": 0.5,
        "prompt_chunk_size": 4,
        "show_realtime": True,
        "save_visualizations": True,
        "visualization_dir": "X:/ë°•ì°½í˜„/pipe_lower_part/data/result",
        "device": "auto",
    }
    
    try:
        # í™˜ê²½ ì„¤ì •
        device = setup_environment(HF_TOKEN, DATASET_CONFIG["device"])
        
        # ========== 1ë‹¨ê³„: ë™ì˜ìƒ â†’ JPEGImages ì¶”ì¶œ ==========
        if DATASET_CONFIG["video_source"] is not None:
            print("\n" + "ğŸ¬ " * 20)
            print("1ë‹¨ê³„: ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ")
            print("ğŸ¬ " * 20 + "\n")
            
            extract_frames_from_videos(
                video_source=DATASET_CONFIG["video_source"],
                jpeg_output_dir=DATASET_CONFIG["jpeg_output_dir"],
                fps_extraction=DATASET_CONFIG["fps_extraction"],
                verbose=True
            )
            
            print("\n" + "âœ“ " * 20)
            print("1ë‹¨ê³„ ì™„ë£Œ: í”„ë ˆì„ ì¶”ì¶œ ì„±ê³µ!")
            print("âœ“ " * 20 + "\n")
            
            # image_sourceë¥¼ jpeg_output_dirë¡œ ìë™ ì„¤ì •
            DATASET_CONFIG["image_source"] = DATASET_CONFIG["jpeg_output_dir"]
        
        # ========== 2ë‹¨ê³„: JPEGImages â†’ YOLO ë¼ë²¨ ìƒì„± ==========
        print("\n" + "ğŸ·ï¸ " * 20)
        print("2ë‹¨ê³„: YOLO ë¼ë²¨ ìƒì„±")
        print("ğŸ·ï¸ " * 20 + "\n")
        
        # ëª¨ë¸ ë¡œë“œ
        model = load_model(bpe_path, device)
        
        # Transform & PostProcessor ìƒì„±
        transform = create_transforms()
        postprocessor = create_postprocessor(DATASET_CONFIG["detection_threshold"], device)
        
        # YOLO ë°ì´í„°ì…‹ ìƒì„±
        create_yolo_dataset(
            image_source=DATASET_CONFIG["image_source"],
            output_dir=DATASET_CONFIG["label_dir"],
            class_mapping=DATASET_CONFIG["class_mapping"],
            bpe_path=bpe_path,
            prompts=None,
            model=model,
            transform=transform,
            postprocessor=postprocessor,
            detection_threshold=DATASET_CONFIG["detection_threshold"],
            device=device,
            prompt_chunk_size=DATASET_CONFIG["prompt_chunk_size"],
            verbose=True,
            show_realtime=DATASET_CONFIG["show_realtime"],
            save_visualizations=DATASET_CONFIG["save_visualizations"],
            visualization_dir=DATASET_CONFIG["visualization_dir"]
        )
        
        print("\n" + "=" * 60)
        print("âœ“ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)
        
        # ìµœì¢… ìš”ì•½
        if DATASET_CONFIG["video_source"] is not None:
            print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
            print(f"  1. ë™ì˜ìƒ ì†ŒìŠ¤: {DATASET_CONFIG['video_source']}")
            print(f"  2. ì¶”ì¶œ FPS: {DATASET_CONFIG['fps_extraction']}")
            print(f"  3. JPEGImages: {DATASET_CONFIG['jpeg_output_dir']}")
            print(f"  4. YOLO ë¼ë²¨: {DATASET_CONFIG['label_dir']}")
            if DATASET_CONFIG["save_visualizations"]:
                print(f"  5. ì‹œê°í™”: {DATASET_CONFIG['visualization_dir']}")
        
    except KeyboardInterrupt:
        print("\n\nâš  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()